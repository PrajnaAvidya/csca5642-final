{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tuning Notebook\n",
    "\n",
    "This is the entire hyperparameter tuning script condensed into a two cells so I can copy/paste it to a Jupyter notebook on vast.ai."
   ],
   "id": "c3428e131922d087"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# for multi-gpu instances this picks which GPU the notebook uses\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "!pip install pandas\n",
    "!pip install librosa\n",
    "!pip install matplotlib\n",
    "!pip install torch_fidelity\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import hashlib\n",
    "import tempfile\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch_fidelity import calculate_metrics\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scipy.io.wavfile import write as wavwrite\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if not os.path.exists(\"/workspace/Data/genres_original/\"):\n",
    "    !unzip -o \"gtzan.zip\" -d \"/workspace/\"\n",
    "else:\n",
    "    print(\"Data already exists, skipping unzip\")\n",
    "base_path = '/workspace/Data/genres_original/'\n",
    "target_genre = 'disco'\n",
    "sample_rate = 16000\n",
    "genre_dir = os.path.join(base_path, target_genre)\n",
    "wav_paths = sorted(glob.glob(f\"{genre_dir}/*.wav\"))\n",
    "print(f\"Loading {target_genre:10s}... {len(wav_paths)} files\")\n",
    "all_audio = {}\n",
    "genre_audio = []\n",
    "bad_files = []\n",
    "for wav_path in tqdm(wav_paths):\n",
    "    try:\n",
    "        waveform, sample_rate = librosa.load(wav_path, sr=sample_rate, mono=True)\n",
    "        genre_audio.append(waveform)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {wav_path}: {e}\")\n",
    "        bad_files.append(wav_path)\n",
    "all_audio[target_genre] = genre_audio\n",
    "print(f\"{len(bad_files)} files discarded.\")\n",
    "\n",
    "def chop_audio_segments(segment_length=5, overlap_seconds=0):\n",
    "    np.random.seed(42)\n",
    "    segment_samples = int(sample_rate * segment_length)\n",
    "    overlap_samples = int(sample_rate * overlap_seconds)\n",
    "    genre_segments = {}\n",
    "\n",
    "    for genre, tracks in all_audio.items():\n",
    "        segments = []\n",
    "        for track_idx, y in enumerate(tracks):\n",
    "            total_samples = len(y)\n",
    "            for start in range(0, total_samples - segment_samples + 1, segment_samples - overlap_samples):\n",
    "                chunk = y[start:start + segment_samples]\n",
    "                if len(chunk) < segment_samples:\n",
    "                    continue\n",
    "                if np.abs(chunk).max() < 1e-4:\n",
    "                    continue\n",
    "                segments.append(chunk.copy())\n",
    "        np.random.shuffle(segments)\n",
    "        genre_segments[genre] = segments\n",
    "        print(f\"Genre {genre:10}: {len(segments)} segments ({segment_length}s, {overlap_seconds}s overlap)\")\n",
    "\n",
    "    return genre_segments\n",
    "\n",
    "class AudioSegmentDataset(Dataset):\n",
    "    def __init__(self, genre_segments_dict, genre, normalize=True, as_channels=True):\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        segments = genre_segments_dict[genre]\n",
    "        self.samples.extend(segments)\n",
    "        self.labels.extend([genre] * len(segments))\n",
    "        self.normalize = normalize\n",
    "        self.as_channels = as_channels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.samples[idx].astype(np.float32)\n",
    "        if self.normalize:\n",
    "            mx = np.max(np.abs(x))\n",
    "            x = x / (mx if mx>0 else 1.0)\n",
    "        if self.as_channels:\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "        x = torch.from_numpy(x)\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "def save_waveform_and_spectrogram(wave, sample_rate, fname, spec_fname=None):\n",
    "    int16 = np.int16(wave * 32767)\n",
    "    wavwrite(fname, sample_rate, int16)\n",
    "    if spec_fname:\n",
    "        S = librosa.feature.melspectrogram(y=wave, sr=sample_rate, n_mels=64, fmax=8000)\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "        plt.figure(figsize=(5, 2))\n",
    "        librosa.display.specshow(S_dB, sr=sample_rate, x_axis='time', y_axis='mel', cmap='magma')\n",
    "        plt.title('')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(spec_fname)\n",
    "        plt.close()\n",
    "\n",
    "def audio_to_melspec_img(audio, sample_rate=16000, n_mels=64, fmax=8000):\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=n_mels, fmax=fmax)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    S_norm = (S_dB - S_dB.min()) / (S_dB.max() - S_dB.min() + 1e-8)\n",
    "    S_img = torch.tensor(S_norm).unsqueeze(0)\n",
    "    return S_img\n",
    "\n",
    "def compute_fid_on_spectrograms_torchfidelity(gen_samples, real_segments,sample_rate=16000, n_mels=64, n_per_set=100, device='cuda'):\n",
    "    num = min(n_per_set, len(gen_samples), len(real_segments))\n",
    "    fake_subset = gen_samples[:num]\n",
    "    real_subset = random.sample(real_segments, num)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as real_dir, tempfile.TemporaryDirectory() as fake_dir:\n",
    "        for i, wav in enumerate(real_subset):\n",
    "            img_tensor = audio_to_melspec_img(wav, sample_rate, n_mels)\n",
    "            save_image(img_tensor, os.path.join(real_dir, f\"real_{i}.png\"))\n",
    "        for i, wav in enumerate(fake_subset):\n",
    "            img_tensor = audio_to_melspec_img(wav, sample_rate, n_mels)\n",
    "            save_image(img_tensor, os.path.join(fake_dir, f\"fake_{i}.png\"))\n",
    "\n",
    "        metrics = calculate_metrics(\n",
    "            input1=fake_dir, input2=real_dir,\n",
    "            cuda=device == 'cuda', isc=False, fid=True, kid=False, verbose=False\n",
    "        )\n",
    "    return metrics['frechet_inception_distance']\n",
    "\n",
    "def save_audio_clips_to_folder(clips, sample_rate, folder):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for i, clip in enumerate(clips):\n",
    "        arr = clip.cpu().numpy() if hasattr(clip, \"cpu\") else clip\n",
    "        if arr.ndim > 1:\n",
    "            arr = arr.squeeze()\n",
    "        arr = np.clip(arr, -1, 1)\n",
    "        wavwrite(os.path.join(folder, f\"{i}.wav\"), sample_rate, (arr * 32767).astype(np.int16))\n",
    "\n",
    "def hash_hyperparams(hyperparams):\n",
    "    hstr = json.dumps(hyperparams, sort_keys=True)\n",
    "    return hashlib.md5(hstr.encode('utf-8')).hexdigest()[:8]\n",
    "\n",
    "def train_wavegan(\n",
    "        run_id,\n",
    "        audio_segments,\n",
    "        netG,\n",
    "        netD,\n",
    "        train_loader,\n",
    "        epochs,\n",
    "        hyperparams,\n",
    "        device,\n",
    "        genre,\n",
    "        checkpoint_interval=10,\n",
    "):\n",
    "    output_dir=f'/workspace/training_outputs/{run_id}/'\n",
    "\n",
    "    lr_g = hyperparams.get('lr_g', 0.0001)\n",
    "    lr_d = hyperparams.get('lr_d', 0.0001)\n",
    "    beta1 = hyperparams.get('beta1', 0.5)\n",
    "    noise_amplitude = hyperparams.get('noise_amplitude', 0.05)\n",
    "    latent_dim = hyperparams.get('latent_dim', 100)\n",
    "\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    optG = optim.Adam(netG.parameters(), lr=lr_g, betas=(beta1, 0.999))\n",
    "    optD = optim.Adam(netD.parameters(), lr=lr_d, betas=(beta1, 0.999))\n",
    "    netG.to(device)\n",
    "    netD.to(device)\n",
    "    loss_curves = {\"gen\": [], \"disc\": []}\n",
    "    fid_history = []\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    sample_dir = os.path.join(output_dir, f'{genre}_samples')\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "    epoch_times = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        g_losses, d_losses = [], []\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        for batch in train_loader:\n",
    "            real_audio, _ = batch\n",
    "            batch_size = real_audio.size(0)\n",
    "            real_audio = real_audio.to(device)\n",
    "            noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "            fake_audio = netG(noise)\n",
    "\n",
    "            label_real = torch.FloatTensor(batch_size).uniform_(0.8, 1.0).to(device)\n",
    "            label_fake = torch.FloatTensor(batch_size).uniform_(0.0, 0.2).to(device)\n",
    "\n",
    "            real_audio_noisy = real_audio + noise_amplitude * torch.randn_like(real_audio)\n",
    "            fake_audio_noisy = fake_audio + noise_amplitude * torch.randn_like(fake_audio)\n",
    "\n",
    "            netD.zero_grad()\n",
    "            out_real = netD(real_audio_noisy)\n",
    "            loss_real = criterion(out_real, label_real)\n",
    "            out_fake = netD(fake_audio_noisy.detach())\n",
    "            loss_fake = criterion(out_fake, label_fake)\n",
    "            loss_D = (loss_real + loss_fake) / 2\n",
    "            loss_D.backward()\n",
    "            optD.step()\n",
    "\n",
    "            netG.zero_grad()\n",
    "            output_gen = netD(fake_audio)\n",
    "            loss_G = criterion(output_gen, label_real)\n",
    "            loss_G.backward()\n",
    "            optG.step()\n",
    "\n",
    "            d_losses.append(loss_D.item())\n",
    "            g_losses.append(loss_G.item())\n",
    "\n",
    "        mean_g = np.mean(g_losses)\n",
    "        mean_d = np.mean(d_losses)\n",
    "        loss_curves[\"gen\"].append(mean_g)\n",
    "        loss_curves[\"disc\"].append(mean_d)\n",
    "\n",
    "        if (epoch % checkpoint_interval == 0) or (epoch==epochs):\n",
    "            # just save the audio and waveform files for now, FID and FAD will be analyzed later on my computer\n",
    "            netG.eval()\n",
    "            with torch.no_grad():\n",
    "                fixed_noise = torch.randn(5, latent_dim, device=device)\n",
    "                samples = netG(fixed_noise).cpu().numpy()\n",
    "                for i, sample in enumerate(samples):\n",
    "                    save_waveform_and_spectrogram(sample[0], sample_rate=16000, fname=os.path.join(sample_dir, f\"ep{epoch}_samp{i}.wav\"), spec_fname=os.path.join(sample_dir, f\"ep{epoch}_samp{i}_spec.png\"))\n",
    "            print(f'Epoch {epoch} | Gen loss: {mean_g:.4f} | Disc loss: {mean_d:.4f}')\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        avg_time_per_epoch = np.mean(epoch_times)\n",
    "        epochs_remaining = epochs - epoch\n",
    "        eta_minutes = (epochs_remaining * avg_time_per_epoch) / 60\n",
    "        if (epoch % checkpoint_interval == 0) or (epoch==epochs):\n",
    "            print(f\"Estimated time remaining: {eta_minutes:.2f} min\")\n",
    "\n",
    "    # save models/optimizers when done\n",
    "    ckpt_dir = os.path.join(output_dir, \"checkpoints\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'netG_state_dict': netG.state_dict(),\n",
    "        'netD_state_dict': netD.state_dict(),\n",
    "        'optG_state_dict': optG.state_dict(),\n",
    "        'optD_state_dict': optD.state_dict(),\n",
    "        'hyperparams': hyperparams,\n",
    "    }, os.path.join(ckpt_dir, f'checkpoint_epoch{epoch}.pt'))\n",
    "\n",
    "    return loss_curves\n",
    "\n",
    "class WaveGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, gf_dim=64, norm='batch'):\n",
    "        super().__init__()\n",
    "        self.project = nn.Linear(latent_dim, 16 * gf_dim * 16)\n",
    "        self.init_channels = 16 * gf_dim\n",
    "        self.init_t = 16\n",
    "\n",
    "        NormLayer = nn.InstanceNorm1d if norm=='instance' else nn.BatchNorm1d\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose1d(16*gf_dim, 8*gf_dim, 25, stride=4, padding=11, output_padding=1),\n",
    "            NormLayer(8*gf_dim), nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(8*gf_dim, 4*gf_dim, 25, 4, 11, 1),\n",
    "            NormLayer(4*gf_dim), nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(4*gf_dim, 2*gf_dim, 25, 4, 11, 1),\n",
    "            NormLayer(2*gf_dim), nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(2*gf_dim, gf_dim, 25, 4, 11, 1),\n",
    "            NormLayer(gf_dim), nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(gf_dim, 1, 25, 4, 11, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.project(z)\n",
    "        x = x.view(z.size(0), self.init_channels, self.init_t)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "class WaveGANDiscriminator(nn.Module):\n",
    "    def __init__(self, df_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(1, df_dim, 25, stride=4, padding=11), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(df_dim, 2*df_dim, 25, 4, 11), nn.BatchNorm1d(2*df_dim), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(2*df_dim, 4*df_dim, 25, 4, 11), nn.BatchNorm1d(4*df_dim), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(4*df_dim, 8*df_dim, 25, 4, 11), nn.BatchNorm1d(8*df_dim), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(8*df_dim, 16*df_dim, 25, 4, 11), nn.BatchNorm1d(16*df_dim), nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*df_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.final(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def random_hyperparam_sample():\n",
    "    return {\n",
    "        \"lr_g\": 10 ** random.uniform(-4.5, -3),\n",
    "        \"lr_d\": 10 ** random.uniform(-4.5, -3),\n",
    "        \"beta1\": random.choice([0.3, 0.5, 0.7]),\n",
    "        \"noise_amplitude\": random.choice([0.005, 0.01, 0.025, 0.05]),\n",
    "        \"latent_dim\": 100,\n",
    "        \"norm\": random.choice(['batch', 'instance']),\n",
    "        \"batch_size\": random.choice([8, 16, 32]),\n",
    "    }\n",
    "\n",
    "def run_random_search(num_trials, genre, epochs, checkpoint_interval, data_segments, custom_hyperparams=None):\n",
    "    finished = []\n",
    "    for i in range(num_trials):\n",
    "        hyperparams = custom_hyperparams if custom_hyperparams else random_hyperparam_sample()\n",
    "        run_id = hash_hyperparams(hyperparams)\n",
    "        print('-' * 60)\n",
    "        print(f\"[{i+1}/{num_trials}] Starting trial {run_id}\")\n",
    "        print(json.dumps(hyperparams, indent=2))\n",
    "        dataset = AudioSegmentDataset(data_segments, genre)\n",
    "        train_loader = DataLoader(dataset, batch_size=hyperparams['batch_size'], shuffle=True, drop_last=True, num_workers=4, pin_memory=True)\n",
    "        netG = WaveGANGenerator(latent_dim=hyperparams['latent_dim'], gf_dim=64, norm=hyperparams['norm'])\n",
    "        netD = WaveGANDiscriminator(df_dim=64)\n",
    "        netG.apply(weights_init)\n",
    "        netD.apply(weights_init)\n",
    "\n",
    "        hp = {k:v for k,v in hyperparams.items()}\n",
    "        hp['run_id'] = run_id\n",
    "        # save hyperparams json\n",
    "        os.makedirs('/workspace/training_outputs/', exist_ok=True)\n",
    "        with open(f'/workspace/training_outputs/{run_id}.json', 'w') as f:\n",
    "            json.dump(hyperparams, f, indent=4)\n",
    "        loss_curves = train_wavegan(run_id, data_segments, netG, netD, train_loader, epochs=epochs, hyperparams=hp, device=device, genre=genre, checkpoint_interval=checkpoint_interval)\n",
    "        summary = {\n",
    "            \"run_id\": run_id,\n",
    "            \"hyperparams\": json.dumps(hyperparams),\n",
    "            \"final_gen_loss\": loss_curves['gen'][-1] if loss_curves['gen'] else None,\n",
    "            \"final_disc_loss\": loss_curves['disc'][-1] if loss_curves['disc'] else None,\n",
    "        }\n",
    "        finished.append(summary)\n",
    "        log_file = f'/workspace/training_outputs/{run_id}.csv'\n",
    "        pd.DataFrame([summary]).to_csv(log_file, mode='a', header=not os.path.exists(log_file), index=False)\n",
    "        print(f\"Completed trial {run_id}\")\n",
    "\n",
    "    return finished\n",
    "\n",
    "audio_segments_1s = chop_audio_segments(16384 / sample_rate, overlap_seconds=0.512)"
   ],
   "id": "a9464bab16a2e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# run random search (for finding hyperparams)\n",
    "start_time = time.time()\n",
    "results = run_random_search(num_trials=20, genre=target_genre, epochs=500, checkpoint_interval=20, data_segments=audio_segments_1s)\n",
    "run_time_seconds = time.time() - start_time\n",
    "print(f'Finished in {int(run_time_seconds)} seconds')"
   ],
   "id": "9c04c5b5be098216"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
